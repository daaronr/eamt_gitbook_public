# pre-giving-tuesday-email split test (+)

_24 Nov 2021 -- adapting this to the_ [trial-reporting-template.md](../../trial-reporting-template.md "mention") _... incorporating the_ [preregistration\_oftw\_pre\_gt.md](preregistration\_oftw\_pre\_gt.md "mention") _and the internal plan notes (below)_

_Note: Some of the below overlaps the content in_ [preregistration\_oftw\_pre\_gt.md](preregistration\_oftw\_pre\_gt.md "mention")\_\_

## General idea, main 'hypothesis' <a href="#general-idea-main-hypothesis" id="general-idea-main-hypothesis"></a>

**Are effectiveness-minded (EA-adjacent) donors and pledgers more motivated to donate by**

1. "A": (non-quantitative) **presentation of impact and effectiveness** (as in standard OftW pitch)
2. "B": **Emotional appeals and 'identified victim' images**

_In the context of **One for The World**'s (OFTW) 'giving season upselling campaign', potentially generalizable to other contexts._

_**Academic framing**_: "Does the Identifiable Victims Effect (see e.g., the meta-analysis by Lee and Feeley, 2016) also motivate the most analytical and committed donors?"

## Background and context

**One for The World**'s (OFTW) 'giving season upselling campaign''

> 10 emails total over the course of November were sent in preparation for GivingTuesday.

### Point of contact (at organization running trial)

[Chloë Cudaback](https://app.gitbook.com/u/wTqRK0aZqff8Tbm0WPvAj34i4k03 "mention")\
\
Academic-linked authors: David Reinstein, Josh Lewis, potentially others

### Timing of trial

Targeted dates: November 10, November 18, November 23, all in 2021, but these may be delayed for feasibility

### Digital location where project 'lives' (planning, material, data)

Present Gitbook, Google doc linked below, preregistration (OSF), and ??

### Environment/context for trial

Emails\
... to existing OftW pledgers (asking for additional donations in Giving Season)

> All 10 emails had the same CTA: make an additional $100 donation for the giving season/GivingTuesday on top of their recurring monthly pledge donation.

### **Participant universe and sample size**

Roughly 4000 participants, as described\\

A series of three campaign emails will be sent out by OftW to their regular email lists, to roughly 4000 participants, as described

### Key treatment(s)

Basically: (Chloe's description)

* A list of \~4500 contacts (activated pledgers) was split into two treatment groups.
* Treatment Group A received emails that were focused on the contact's impact
* while Treatment Group B received emails that were focused on individual stories of beneficiaries

See [#treatment-specifics-i.e.-experimental-conditions](preregistration\_oftw\_pre\_gt.md#treatment-specifics-i.e.-experimental-conditions "mention")

### Treatment assignment procedure

.

See [#4-how-many-and-which-conditions-will-participants-be-assigned-to](preregistration\_oftw\_pre\_gt.md#4-how-many-and-which-conditions-will-participants-be-assigned-to "mention")

### **Outcome data**

Targeting: Donation incidence and amount in the relevant 'giving season' and over the next year, specifically described in prereg under...[#3-describe-the-key-dependent-variable-s-specifying-how-they-will-be-measured.](preregistration\_oftw\_pre\_gt.md#3-describe-the-key-dependent-variable-s-specifying-how-they-will-be-measured. "mention")

\
**Data storage/form:**

* MailChimp data (Chloe sharing this),
* Reports on donations (Kennan is gathering this)

### **Optional/suggested additions**

_Planned analysis methods, preregistration link: see_ [preregistration\_oftw\_pre\_gt.md](preregistration\_oftw\_pre\_gt.md "mention") and [#5-specify-exactly-which-analyses-you-will-conduct-to-examine-the-main-question-hypothesis.](preregistration\_oftw\_pre\_gt.md#5-specify-exactly-which-analyses-you-will-conduct-to-examine-the-main-question-hypothesis. "mention") within\
\_\_

_Cost of running trial/promotion: Time costs only (as far as I know)_

## Proposed/implementing design (language)

([Link)](https://docs.google.com/document/d/1VyAtfJ2bFaQBfQVlflIdsN29Otr7g8YjjihXVfBv7UM/edit?usp=sharing)

{% embed url="https://docs.google.com/document/d/1VyAtfJ2bFaQBfQVlflIdsN29Otr7g8YjjihXVfBv7UM/edit?usp=sharing" %}

## Pre-registration work

Pre-registered on OSF in 'AsPredicted' format, content incorporated here [preregistration\_oftw\_pre\_gt.md](preregistration\_oftw\_pre\_gt.md "mention")

{% embed url="https://github.com/daaronr/effective_giving_market_testing/blob/main/contexts-and-environments-for-testing/one-for-the-world/preregistration_oftw_pre_gt.pdf" %}

## Preliminary results

_(From message from Chloe; description of experiment integrated above)_

> Preliminary analysis shows that even with the one high-performing outlier removed from the dataset, **those emails that appealed to emotional storytelling performed better (higher in-email click rate) than those that were impact-focused**.

> Further analysis that details the way that different contact demographics interacted with each email treatment type will also be done, as well as an analysis of clicks converted to completed donations.
>
>

### **Evidence on donations**&#x20;

_(preliminary; we are awaiting further donations in the giving season) ..._&#x20;

{% hint style="info" %}
_This is 'hard-coded' below. I intend to replace this with a link or embed of a dynamic document (Rmarkdown)._\
__\
_The quantitative analysis itself, stripped of any context and connection to OftW, is hosted_ [_HERE_ ](https://rethinkpriorities.github.io/methodology-statistics-design/inference-and-rough-equivalence-testing-with-binomial-outcomes.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes)__
{% endhint %}

{% embed url="https://rethinkpriorities.github.io/methodology-statistics-design/inference-and-rough-equivalence-testing-with-binomial-outcomes.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes" %}
_the analysis as a 'methodological example'; all context removed_
{% endembed %}

**Treatment 1 (Impact)**:  We record

* 8 unique emails donating, 26 donations in total,
* &#x20;worth $5200  in total
* &#x20;1345 unique emails listed as getting  ‘control’ treatment 1

**Treatment 2 (Emotional storytelling):**

* 6 unique emails donating, 28 donations so far
* worth $7500 in total.
* &#x20;1190 unique emails listed for treatment 2

(If I believe my ‘unique emails count’) that implies

* an 0.59% ‘conversion’ rate for T1 - Control
* a 0.50% conversion rate for T2 - Emotion/Story\


&#x20;**‘Initial impressions of preliminary outcomes’**

* The conversion rates are rather low (0.5%) … but maybe high enough to justify sending these emails? I’m not sure.
* The rates are similar across the treatments
* Given the low conversion rates we don’t have too much power to rule out ‘proportionally large’ differences in conversion rates (or average amounts raised) between treatments … &#x20;

![](<../../../.gitbook/assets/image (3).png>)



The figure above seems like a good summary of the ‘results so far’ on ‘what we can infer about relative incidence rates’,  presuming I understand the situation correctly …I plot\
\
Y-axis: ’how likely would a difference in donations ‘as small or smaller in magnitude’” than we see in the data between the incidence (8 vs 6 people donating) be” … against\
\
X-axis: if the “true difference in incidence rates” were of these magnitudes_Takeaway:_

* our data is consistent with ‘no difference’ (of course) … but it's also consistent with ‘a fairly large difference in incidence’
* E.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 20% chance of seeing a difference as small as the one we see (of 8 versus 6)
* We can reasonably ‘rule out’ differences of maybe 2.5x or greater
* Main point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either direction … at least not yet. I hope that combined with other evidence, we will be able to infer more



