# pre-giving-tuesday-email split test (+)

{% hint style="info" %}
**To do:** This is being moved to the public gitbook [(edit link here)](https://app.gitbook.com/s/a3YtWoUiYYfiEQrBNztC/partner-organizations-and-trials/one-for-the-world-oftw/pre-giving-tuesday-email-split-test-+).&#x20;

* Reconcile these versions
* replace the present information with a link (plus any information that would need to be redacted from the public version)
{% endhint %}

__

__

## General idea, main 'hypothesis' <a href="#general-idea-main-hypothesis" id="general-idea-main-hypothesis"></a>

**Are effectiveness-minded (EA-adjacent) donors and pledgers more motivated to donate by**

1. "A": (non-quantitative) **presentation of impact and effectiveness** (as in standard OftW pitch)
2. "B": **Emotional appeals and 'identified victim' images**

_In the context of **One for The World**'s (OFTW) 'giving season upselling campaign', potentially generalizable to other contexts._

_**Academic framing**_: "Does the Identifiable Victims Effect (see e.g., the meta-analysis by Lee and Feeley, 2016) also motivate the most analytical and committed donors?"

## Background and context

**One for The World**'s (OFTW) 'giving season upselling campaign''

> 10 emails total over the course of November were sent in preparation for GivingTuesday

### Point of contact (at organization running trial)

[Chloë Cudaback](https://app.gitbook.com/u/wTqRK0aZqff8Tbm0WPvAj34i4k03 "mention")\
\
Academic-linked authors: David Reinstein, Josh Lewis, potentially others

### Timing of trial

Targeted dates: November 10, November 18, November 23, all in 2021, but these may be delayed for feasibility&#x20;



{% hint style="info" %}
Update -- four email dates are recorded in the data&#x20;
{% endhint %}

### Digital location where project 'lives' (planning, material, data)

Present Gitbook, Google doc linked below, preregistration (OSF), and &#x20;

{% hint style="info" %}
Update: github/git repo
{% endhint %}

### Environment/context for trial

Emails\
... to existing OftW pledgers (asking for additional donations in Giving Season)

> All 10 emails had the same CTA: make an additional $100 donation for the giving season/GivingTuesday on top of their recurring monthly pledge donation.

__

{% hint style="info" %}
Update -- four email dates are recorded in the data&#x20;
{% endhint %}



### **Participant universe and sample size**

Roughly 4000 participants, as described\\

A series of three campaign emails will be sent out by OftW to their regular email lists, to roughly 4000 participants, as described

### Key treatment(s)

Basically: (Chloe's description)

* A list of \~4500 contacts (activated pledgers) was split into two treatment groups.
* Treatment Group A received emails that were focused on the contact's impact
* while Treatment Group B received emails that were focused on individual stories of beneficiaries

See [Broken link](broken-reference "mention")

### Treatment assignment procedure

See [Broken link](broken-reference "mention")

### **Outcome data**

Targeting: Donation incidence and amount in the relevant 'giving season' and over the next year, specifically described in prereg under...[Broken link](broken-reference "mention")

\
**Data storage/form:**

* MailChimp data (Chloe sharing this),
* Reports on donations (Kennan is gathering this)

### **Optional/suggested additions**

_Planned analysis methods, preregistration link: see_ [Broken link](broken-reference "mention") and [Broken link](broken-reference "mention") within\
\_\_

_Cost of running trial/promotion: Time costs only (as far as I know)_

## Proposed/implementing design (language)

([Link)](https://docs.google.com/document/d/1VyAtfJ2bFaQBfQVlflIdsN29Otr7g8YjjihXVfBv7UM/edit?usp=sharing)

{% embed url="https://docs.google.com/document/d/1VyAtfJ2bFaQBfQVlflIdsN29Otr7g8YjjihXVfBv7UM/edit?usp=sharing" %}

## Pre-registration work

Pre-registered on OSF in 'AsPredicted' format, content incorporated here [Broken link](broken-reference "mention")

{% embed url="https://github.com/daaronr/effective_giving_market_testing/blob/main/contexts-and-environments-for-testing/one-for-the-world/preregistration_oftw_pre_gt.pdf" %}

## Preliminary results

### **Overview:**&#x20;

The Emotion treatment leads to significantly more clicks on the in-email donation link than the standard Impact information treatment. However, we are statistically underpowered to detect a difference in actual donations. More evidence is needed&#x20;

Chloe:  **those emails that appealed to emotional storytelling performed better (higher in-email click rate) than those that were impact-focused**.

**DR, update:** I confirm that this is indeed the case, and this is statistically significant in further analysis.&#x20;

> ### **Evidence on donations**

_(preliminary; we are awaiting further donations in the giving season) ..._

{% hint style="info" %}
_This is 'hard-coded' below. I intend to replace this with a link or embed of a dynamic document (Rmarkdown)._\
\
_**The quantitative analysis itself, stripped of any context and connection to OftW, is hosted**_** ** [_**HERE**_](https://rethinkpriorities.github.io/methodology-statistics-design/inference-and-rough-equivalence-testing-with-binomial-outcomes.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes)
{% endhint %}

{% embed url="https://rethinkpriorities.github.io/methodology-statistics-design/inference-and-rough-equivalence-testing-with-binomial-outcomes.html#how-likely-are-proportions-this-similar-under-different-size-true-effect-sizes" %}
_the analysis as a 'methodological example'; all context removed_
{% endembed %}



****

{% hint style="info" %}
**Note:**   We may wish to treat the 'email send' as the denominator, as the differig subject seemed to have led to  a different number of opens
{% endhint %}

****

****

**Treatment 1 (Impact)**: We record

* 1405 unique emails listed as opening a ‘control’ treatment email
* 29 members clicking on the donation link in an email at least once (2.1% of openers)
* 15 members making some one-time donation in this period (about 0.11% of openers, 0.075% of total)
* 8 members emails donating (likely) through the link (0.057%/0.04%)



{% hint style="info" %}
**Treatment 2 (Emotional storytelling):**
{% endhint %}

* 1190 unique emails listed as opening an email (a significantly lower 'open rate', assuming the same shares of members were sent each set of treatment email)&#x20;
* 56 members clicking on the donation link in an email at least once (4.7% of openers)
* 11 members making some one-time donation in this period (about 0.9% of openers, about 0.055% of total)
* 9 unique emails donating (likely) through the link (0.08%/0.045%)



{% hint style="info" %}
**Note:**   We may wish to treat the 'email send' as the denominator, as the differing subject seemed to have led to  a different number of opens
{% endhint %}

****



**‘Initial impressions of preliminary outcomes’**

* The conversion rates are rather low (0.5%) … but maybe high enough to justify sending these emails? I’m not sure.
* While people are more likely to O_pen_ at least one Impact email, they are more likely to _Click to donate_ at least once if assigned the Emotion email
* But we can't say much for _actual donations._
* Given the low conversion rates we don’t have too much power to rule out ‘proportionally large’ differences in conversion rates (or average amounts raised) between treatments …

![](<../../../.gitbook/assets/image (8).png>)

The figure above seems like a good summary of the ‘results so far’ on ‘what we can infer about relative incidence rates’, presuming I understand the situation correctly …I plot\
\
Y-axis: ’how likely would a difference in donations ‘as small or smaller in magnitude’” than we see in the data between the incidence … against\
\
X-axis: if the “true difference in incidence rates” were of these magnitudes

_Here:_

* our data is consistent with ‘no difference’ (of course) … but it's also consistent with ‘a fairly large difference in incidence’
* E.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 33% chance or so of seeing a difference as small as the one we see&#x20;
* We can reasonably ‘rule out’ differences of maybe 2.5x or greater
* Main point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either direction about donationse
