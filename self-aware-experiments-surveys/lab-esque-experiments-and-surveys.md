# Discussion: "Lab-esque" experiments/surveys

## "A large-scale online experiment" (participants-aware)


**Reinstein comment:**


We are focused on doing field experiments and marketing trials. However, we could also consider 'laboratory-type experiments' and surveys, with small or hypothetical donation/action choices, earnings determined by the experimenters, and, perhaps most importantly, participants aware they are in an experiment. Bastian discusses an ambitious program for this below. I think this has value and can complement [our lab efforts (see my comments at the bottom). ](lab-esque-experiments-and-surveys.md#reinstein-further-comments) However, I don't want this to derail us from our marketing and field experiments in key EA-relevant settings (for reasons discussed at bottom).&#x20;


### Notes from Bastian Jaeger (DR re-wording in part)

* Considering the possibility of running online lab experiments to test the effectiveness of different interventions.&#x20;
* Considering, addressing criticisms of the value of these

#### My proposal:&#x20;

* Run a high-powered online experiment \[DR; 'series of comparable experiments'] that compares the effectiveness of (ideally many) different interventions in stimulating effective giving
* Insights may inform future field experiments

#### General thoughts

* Some limitations of prior work can be addressed with better study designs [DR: which in particular?]
* Generalizability: perhaps rather low, but probably not zero&#x20;
* Planning field experiments and interventions: Better to base design choices on results from online experiments than on personal experience or hunches. [DR: reasonable but experience can also be informative]

### Key issues, responses

1. **External generalizability:** How much does behavior in '(contrived/artificial/lab-based)' experiments tell us about behavior in the real world? [DR: or about *responses* to different presentations?]

- Responses may systematically differ in these settings from that of real-world settings for reasons discussed below

- But 'the same psychological mechanisms operate in each case'
  - Especially basic perceptual or physiological processes
  - Less so for 'complex social phenomena' [DR: what's the simple argument for 'why less so'?] ... but still some of the same psychological processes matter [DR: what's the justification for this claim?]

DR: I expect some things to carry over well, such as familiarity ('have you heard of givewell'), stated beliefs about objective facts ('where do you think you are in the world income distribution'), recognition/understanding of treatment language and marketing wording, and 'how people respond outwardly or say people should respond.'  But when people donate a small amount  in a lab experiment, or, even more, state how they would behave in a hypothetical situation, I think the choices are often expressive, or statements of 'desired responses', and may not tell us much about how they would respond in large-scale real-world situations.

2. **'What is the best outcome variable to use?'**

- Real world giving happens through many channels; experiments cannot duplicate all of these meaningfully

* Forced choice versus dictator game" Do participants receive money that has to be allocated between different charities or can they also keep money for themselves?

- Windfall versus real-effort earnings: Do participants “earn” money through a different task or are they simply handed the money?

[DR: note, that's not about an outcome variable per se]

- Choices among charities or single-charity: Do participants see one effective charity that they could donate to or not or do they see a menu of different charities, some effective some not?


3. **Many existing studies are underpowered (statistical power)**

Online platforms like Prolific make it easy to recruit large sample sizes and enable strong statistical power.

[DR: I agree that this is a limitation of much earlier work especially in psychology. But I have a nuanced view of 'power' I'd like to discuss.]

4. ** Many existing studies don’t measure actual donations with actual money (incentives)**

I have mixed views on this. Most economists’ have strong views pro incentivization. But the empirical evidence on whether decision processes differ when choices are incentivized or not is mixed (I think David knows more about this).

[DR: there are many types of lab choices that don't seem to change much when incentives are added. But others do change, and there is far from a strong case that 'this never matters'. I particularly expect incentives to matter in cases where people are trading off personal gain for social desirability, like in the charitable giving case.]

Moreover, many design choices approved by economists look like they only address the problem cosmetically. Design in which only a fraction of participants receive payments in the end (e.g., lottery designs) or in which everyone receives a really small sum are often approved even though the monetary stakes for the average participant are still extremely low. A conservative [why 'conservative'?]  measure would be to incentivize donation decisions as much as the available funds allow.

[DR: I totally agree]

\

For example, each participant could receive a relatively small amount (e.g., $2) that they could donate or not. Participants could vote on where a larger amount of money (e.g., $1000) should be donated and the charity with the most votes gets it in the end.

[DR: Even that seems like a rather small incentive to me]

**4. Many existing studies only test one or a few interventions**

This makes it difficult to identify the most promising interventions. Making comparisons across different studies is difficult because many other factors differ. I think a single, large-scale study that compares a large number of interventions would be more informative. This would require some restrictions on candidate interventions. For example, interventions should only consist of X words/ one page in the survey/not take longer than 1 minute to process.

[DR: Agree that we need systematic variation, variation should not occur in multiple dimensions in uncontrolled ways.]

**5. The effect of any intervention will [DR: may] depend on small, seemingly inconsequential differences in wording**

One important design feature could be to decide on a set of promising interventions and then create different versions of each intervention (a kind of stimulus sampling). For example, when testing the effect of providing effectiveness information, we could test different ways in which this info is presented: (a) this charity is estimated to be 100x more effective than the average charity, (b) this charity is one of the most effective charities, (c) this charity is particularly effective, for every $X donated, one life will be saved.


[DR, totally agree, and stimulus sampling seems very promising to me. To some extent, we can and should also do this in field experiments.]


**6. Identifying the most promising interventions that are tested**

The space of potential interventions is vast and only some can be tested. I am imagining a multi-step crowdsourced approach, where experts (academics, people from the wider EA community) propose interventions that they expect to be most effective (see this for a similar approach: [https://schwitzsplinters.blogspot.com/2020/06/contest-winner-philosophical-argument.html](https://schwitzsplinters.blogspot.com/2020/06/contest-winner-philosophical-argument.html)). Submitted interventions could be judged on expected effectiveness to identify the X most promising ones. Different versions of the most promising ones could be created (stimulus sampling) before testing the final set.

**7. Incentives to participate**

I imagine there would be enough interest to participate in this project by submitting or judging interventions. As is common with these crowdsourced research projects, everyone could ultimately be an author on the resulting paper. The process could also be gamified a bit. The team with the most effective intervention could win a small prize or a more desired position in the author list. All data could be made openly accessible.

Such a large project would need a core team. I would be happy to push this forward, but I don’t think I would have the time to coordinate everything by myself.

**8. ** Funding

This could be a major obstacle. Testing a large number of interventions with a large number of participants whose decisions are sufficiently incentivized means that this will be a very expensive study. I see a few potential solutions: (1) Apply for (partial) funding from an EA organization. Some organizations may be willing to provide financial support because the insights might be relevant for them (and because a significant chunk of the money will hopefully go to effective charities). (2) Crowdsource. Other researchers might be willing to contribute some of their research funds. (3) Apply for funding through academic channels. This would likely take very long and chances of receiving funds are usually low.



## Reinstein further comments

I think our focus on field trials is is justified because:

1. "Validity" and generalizability: These yield directly-applicable insights in the contexts they are run, and very likely generalizable insights for 'other similar EA orgs and appeals'. They avoid many 'biases and limitations' of lab and 'participant-aware' trials (stake sizes, hypothetical vs real choices, house money, experimenter demand, student or 'weird mturk' subject pools, etc., which are somwhat acknowledged above.)
2. "Neglectedness and tractability": There has been less work on these relative to lab-esque trials, and we have a unique opportunity to run these
3. Cost and knock-on benefits: These are (often) self-financing, and in fact running these trials may generate value in terms of donations and awareness.

Nonetheless, I see a role for lab-esque, online 'participant aware' experiments, both to supplement our field trials, and to directly gain insights.

_I refer here to experiments_

* that we set up and finance (or crowd-fund)
* that are _not_ tied to a field organization or natural setting',
* that may use subject pools with 'known quantified demographics'
* potentially involving 'lab-earned' money, forced choices among charities, or even hypothetical choices and stated beliefs and attitudes

_Possible advantages/benefits of lab-type work_:

1. Participants of interest ('relevant or representative sample') who may _never enter our field sites_
2. Elicit inframarginal choices relevant to people at some distance from EA.

(People who would never give to (e.g.) AMF in a campaign... but we can learn more about what makes them more or less sympathetic to this)

1. Pay for detailed attitudinal responses, free responses, etc.
2. Engineer a 'common testing setting' for careful comparison
3. Pre-test and post-test frames and responses

_Other notes and thoughts:_ I really support this, but I don’t want it to crowd out our drive to get some field experiments done and reported. We did indeed plan things like surveys and hypothetical choices as well as Prolific experiments as part of our broader plan, especially relevant to ‘profiling’. I just want to focus first on getting some marketing and field trials up. So maybe if Bastian and others can work somewhat-independently on this, but as part of our general effort, and we will assist. As I note, these can and should supplement and enhance the field trials... e.g., pre and post-testing of how people see the frames. Also if we are doing the self-aware experiments I’d ideally like to ‘do it better, more systematically, and potentially with adaptive designs’ than the typical academic work.
