# "Lab-esque" experiments and surveys

## "A large-scale online experiment" (Participants aware of experiment)

### Reinstein comment:

We are focused on doing field experiments and trials. I think this is justified because:

1. "Validity" and generalizability: These yield directly-applicable insights in the contexts they are run, and very likely generalizable insights for 'other similar EA orgs and appeals'. They avoid many 'biases and limitations' of lab and 'participant-aware' trials (stake sizes, hypothetical vs real choices, house money, experimenter demand, student or 'weird mturk' subject pools, etc.)
2. "Neglectedness and tractability": There has been less work on these relative to lab-esque trials, and we have a unique opportunity to run these
3. Cost and knock-on benefits: These are (often) self-financing, and in fact running these trials may generate value in terms of donations and awareness.

Nonetheless, I see a role for lab-esque, online 'participant aware' experiments, both to supplement our field trials, and to directly gain insights.

_I refer here to experiments_

* that we set up and finance (or crowd-fund)
* that are _not_ tied to a field organization or natural setting',
* that may use subject pools with 'known quantified demographics'
* potentially involving 'lab-earned' money, forced choices among charities, or even hypothetical choices and stated beliefs and attitudes

_Possible advantages/benefits_:

1. Participants of interest ('relevant or representative sample') who may _never enter our field sites_
2. Elicit inframarginal choices relevant to people at some distance from EA.

(People who would never give to (e.g.) AMF in a campaign... but we can learn more about what makes them more or less sympathetic to this)

1. Pay for detailed attitudinal responses, free responses, etc.
2. Engineer a 'common testing setting' for careful comparison
3. Pre-test and post-test frames and responses

_Other notes and thoughts:_ I really support this, but I don’t want it to crowd out our drive to get some field experiments done and reported. We did indeed plan things like surveys and hypothetical choices as well as Prolific experiments as part of our broader plan, especially relevant to ‘profiling’. I just want to focus first on getting some marketing and field trials up. So maybe if Bastian and others can work somewhat-independently on this, but as part of our general effort, and we will assist. As I note, these can and should supplement and enhance the field trials... e.g., pre and post-testing of how people see the frames. Also if we are doing the self-aware experiments I’d ideally like to ‘do it better, more systematically, and potentially with adaptive designs’ than the typical academic work.

### Notes from Bastian Jaeger

(DR: I plan to read and respond in detail)

I wanted to write down my thoughts and get your reactions on the possibility of running online lab experiments to test the effectiveness of different interventions. I know that many people are (rightfully) skeptical about how informative online experiments (when run like the typical academic study) can be to predict “real-world” behavior, which is why I organized my views as potential criticisms and their rebuttals. Feel free to add to both or to add your thoughts in another format.

My proposal would be to run a high-powered online experiment that compares the effectiveness of (ideally many) different interventions in stimulating effective giving. The insights could be used to inform future field experiments within EA organizations. I see many limitations of typical experiments in this area and I am open to being convinced that the informational value of any study is so low that it would not be worth the money and effort. My current view is somewhat favorable because (a) some limitations of prior studies in this area can be addressed with better study designs and (b) even if the expected generalizability of the results is rather low, it is probably not zero. When deciding which interventions to implement in the field, it might be better to base design choices on results from online experiments than on personal experience or hunches.

**Potential issues and solutions**

1\) Behavior in online experiments tell us nothing about behavior in the “real world” (external validity)

I think this is the biggest issue. Typical experiments likely have low external validity and are therefore relatively uninformative. I wouldn’t go so far as to dismiss all (contrived/artificial/lab-based) experimental work based on this criticism. To an extent, the same psychological mechanisms operate in the lab and in everyday life. For example, we (rightfully) don’t dismiss findings on basic perceptual or physiological processes as uninformative because we know that these processes operate in very similar ways inside and outside the lab. For complex social phenomena, this becomes more problematic, but still, evidence from lab studies may shed \*some\* light on psychological processes that also operate in everyday life.

I think the central issue for this study would be to settle on what is the best DV. Giving in everyday life happens through many different channels and most are difficult to model in an experiment. I can only come up with less-than-ideal options. Important question that need to be answered are:

o Do participants receive money that has to be allocated between different charities or can they also keep money for themselves?

o Do participants “earn” money through a different task or are they simply handed the money?

o Do participants see one effective charity that they cold donate to or not or do they see a menu of different charities, some effective some not?

2\) Many existing studies are underpowered (statistical power)

True. An informative study should be sufficiently powered. Online platforms like Prolific make it easy to recruit large sample sizes.

3\) Many existing studies don’t measure actual donations with actual money (incentives)

I have mixed views on this. Most economists’ have strong views pro incentivization. But the empirical evidence on whether decision processes differ when choices are incentivized or not is mixed (I think David knows more about this). Moreover, many design choices approved by economists look like they only address the problem cosmetically. Design in which only a fraction of participants receive payments in the end (e.g., lottery designs) or in which everyone receives a really small sum are often approved even though the monetary stakes for the average participant are still extremely low. A conservative measure would be to incentivize donation decisions as much as the available funds allow.

For example, each participant could receive a relatively small amount (e.g., $2) that they could donate or not. Participants could vote on where a larger amount of money (e.g., $1000) should be donated and the charity with the most votes gets it in the end.

4\) Many existing studies only test one or a few interventions

This makes it difficult to identify the most promising interventions, Making comparisons across different studies is difficult because many other factors differ. I think a single, large-scale study that compares a large number of interventions would be more informative. This would require some restrictions on candidate interventions. For example, interventions should only consist of X words/ one page in the survey/not take longer than 1 minute to process.

5\) The effect of any intervention will depend on small, seemingly inconsequential differences in wording

One important design feature could be to decide on a set of promising interventions and then create different versions of each intervention (a kind of stimulus sampling). For example, when testing the effect of providing effectiveness information, we could test different ways in which this info is presented: (a) this charity is estimated to be 100x more effective than the average charity, (b) this charity is one of the most effective charities, (c) this charity is particularly effective, for every $X donated, one life will be saved.

6\) Identifying the most promising interventions that are tested

The space of potential interventions is vast and only some can be tested. I am imagining a multi-step crowdsourced approach, where experts (academics, people from the wider EA community) propose interventions that they expect to be most effective (see this for a similar approach: [https://schwitzsplinters.blogspot.com/2020/06/contest-winner-philosophical-argument.html](https://schwitzsplinters.blogspot.com/2020/06/contest-winner-philosophical-argument.html)). Submitted interventions could be judged on expected effectiveness to identify the X most promising ones. Different versions of the most promising ones could be created (stimulus sampling) before testing the final set.

7\) Incentives to participate

I imagine there would be enough interest to participate in this project by submitting or judging interventions. As is common with these crowdsourced research projects, everyone could ultimately be an author on the resulting paper. The process could also be gamified a bit. The team with the most effective intervention could win a small prize or a more desired position in the author list. All data could be made openly accessible.

Such a large project would need a core team. I would be happy to push this forward, but I don’t think I would have the time to coordinate everything by myself.

7\) Funding

This could be a major obstacle. Testing a large number of interventions with a large number of participants whose decisions are sufficiently incentivized means that this will be a very expensive study. I see a few potential solutions: (1) Apply for (partial) funding from an EA organization. Some organizations may be willing to provide financial support because the insights might be relevant for them (and because a significant chunk of the money will hopefully go to effective charities). (2) Crowdsource. Other researchers might be willing to contribute some of their research funds. (3) Apply for funding through academic channels. This would likely take very long and chances of receiving funds are usually low.
